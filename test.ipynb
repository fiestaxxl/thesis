{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import TransformerM, CVAE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "#from train import train_CVAE\n",
    "import numpy as np\n",
    "from data.utils import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecules_input, molecules_output, char, vocab, labels, length = load_data('data/smiles_prop.txt',120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_data = int(len(molecules_input)*0.75)\n",
    "train_molecules_input = molecules_input[0:num_train_data]\n",
    "test_molecules_input = molecules_input[num_train_data:-1]\n",
    "\n",
    "train_molecules_output = molecules_output[0:num_train_data]\n",
    "test_molecules_output = molecules_output[num_train_data:-1]\n",
    "\n",
    "train_labels = labels[0:num_train_data]\n",
    "test_labels = labels[num_train_data:-1]\n",
    "\n",
    "train_length = length[0:num_train_data]\n",
    "test_length = length[num_train_data:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15724/3198622727.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  x = nn.functional.one_hot(torch.tensor([train_molecules_input[i] for i in n], dtype=torch.int64), num_classes=len(vocab))\n"
     ]
    }
   ],
   "source": [
    "n = np.random.randint(len(train_molecules_input), size = 4)\n",
    "x = nn.functional.one_hot(torch.tensor([train_molecules_input[i] for i in n], dtype=torch.int64), num_classes=len(vocab))\n",
    "y = torch.tensor([train_molecules_output[i] for i in n], dtype=torch.int64)\n",
    "l = torch.tensor(np.array([train_length[i] for i in n]), dtype=torch.int64)\n",
    "c = torch.tensor(np.array([train_labels[i] for i in n]).astype(float),dtype=torch.float).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_x_embed = torch.nn.utils.rnn.pack_padded_sequence(input= x_embed, lengths=l, batch_first=True, enforce_sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([[-2.4341,  1.3255, -0.3168,  ..., -0.7388, -0.8555, -0.0082],\n",
       "        [-2.4341,  1.3255, -0.3168,  ..., -0.7388, -0.8555, -0.0082],\n",
       "        [-2.4341,  1.3255, -0.3168,  ..., -0.7388, -0.8555, -0.0082],\n",
       "        ...,\n",
       "        [ 0.7519, -0.2907, -0.1540,  ..., -0.3306, -0.1111, -0.8612],\n",
       "        [ 0.7519, -0.2907, -0.1540,  ..., -0.3306, -0.1111, -0.8612],\n",
       "        [-1.0513,  0.1108,  0.8604,  ...,  1.5877,  0.4810, -0.8234]],\n",
       "       grad_fn=<PackPaddedSequenceBackward0>), batch_sizes=tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "        4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1,\n",
       "        1, 1]), sorted_indices=tensor([0, 2, 3, 1]), unsorted_indices=tensor([0, 3, 1, 2]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed_x_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, emb_dim=300, num_emb=35):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.emb =nn.Embedding(embedding_dim=emb_dim, num_embeddings=num_emb)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.emb(x.argmax(dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim=35, emb_dim=300, num_emb=35, hidden_units=1024, num_layers=3, seq_len=120, cond_dim=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "        self.emb = Embedding(emb_dim=emb_dim,\n",
    "                             num_emb=num_emb)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # x: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        x_emb = self.emb(x)\n",
    "        c = torch.nn.functional.interpolate(c.unsqueeze(1), size=(self.seq_len, self.cond_dim), mode='nearest').squeeze(1)\n",
    "        x_emb = torch.cat([x_emb,c], dim=-1)\n",
    "        outputs, (hidden, cell) = self.lstm(x_emb)\n",
    "        return outputs, (hidden, cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parametrizator(nn.Module):\n",
    "    def __init__(self,hidden_units, latent_dim, num_layers):\n",
    "        super(Parametrizator, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_units\n",
    "        self.latent_size = latent_dim\n",
    "        self.lstm_factor = num_layers\n",
    "\n",
    "        self.mean = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, out_features= self.latent_size)\n",
    "        self.log_variance = torch.nn.Linear(in_features= self.hidden_size * self.lstm_factor, out_features= self.latent_size)\n",
    "\n",
    "    def reparametize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(std)\n",
    "\n",
    "        z = mu + noise * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, hid_state):\n",
    "        enc_h = hid_state.view(hid_state.shape[1], self.hidden_size*self.lstm_factor)\n",
    "        mu = self.mean(enc_h)\n",
    "        log_var = self.log_variance(enc_h)\n",
    "\n",
    "        z = self.reparametize(mu,log_var)\n",
    "        return z, mu, log_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cond_dim, seq_len, latent_dim=120, hidden_units=1024, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.latent_size = latent_dim\n",
    "        self.lstm_factor = num_layers\n",
    "        self.seq_len = seq_len\n",
    "        self.cond_dim = cond_dim\n",
    "\n",
    "        self.init_hidden_decoder = torch.nn.Linear(in_features= self.latent_size, out_features= self.hidden_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=latent_dim+cond_dim,\n",
    "            hidden_size=hidden_units,\n",
    "            num_layers = num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, z, c):\n",
    "        c = torch.nn.functional.interpolate(c.unsqueeze(1), size=(self.seq_len, self.cond_dim), mode='nearest').squeeze(1)\n",
    "\n",
    "        z_inp = z.repeat(1, self.seq_len, 1)\n",
    "        hidden = z.repeat(1,self.num_layers,1)\n",
    "\n",
    "        batch_size = c.shape[0]\n",
    "\n",
    "        z_inp = z_inp.view(batch_size, self.seq_len, self.latent_size)\n",
    "        hidden = hidden.view(self.num_layers, batch_size, self.latent_size)\n",
    "\n",
    "        hidden_decoder = self.init_hidden_decoder(hidden)\n",
    "        hidden_decoder = (hidden_decoder, hidden_decoder)\n",
    "\n",
    "        z_inp = torch.cat([z_inp,c], dim=-1)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(z_inp,hidden_decoder)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, hidden_units, classes):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.classes = classes\n",
    "\n",
    "        self.fc1 = nn.Linear(hidden_units, 256)\n",
    "        self.fc2 = nn.Linear(256,128)\n",
    "        self.fc3 = nn.Linear(128,classes)\n",
    "\n",
    "        self.predictor = nn.Sequential(self.fc1,self.fc2,self.fc3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.predictor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    def __init__(self, cond_dim = 3, hidden_units = 512, num_layers = 3, emb_dim = 300, latent_dim = 256,\n",
    "                 vocab_size = len(vocab), seq_len = 120):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.cond_dim = cond_dim\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = num_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_emb = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.enc = Encoder(input_dim=emb_dim + cond_dim,\n",
    "                           emb_dim=emb_dim,\n",
    "                           num_emb=vocab_size,\n",
    "                           hidden_units=hidden_units,\n",
    "                           num_layers=num_layers,\n",
    "                           seq_len=seq_len,\n",
    "                           cond_dim=cond_dim)\n",
    "\n",
    "        self.param = Parametrizator(hidden_units=hidden_units,\n",
    "                                    latent_dim=latent_dim,\n",
    "                                    num_layers=num_layers)\n",
    "\n",
    "        self.dec = Decoder(cond_dim=cond_dim,\n",
    "                           seq_len=seq_len,\n",
    "                           latent_dim=latent_dim,\n",
    "                           hidden_units=hidden_units,\n",
    "                           num_layers=num_layers)\n",
    "\n",
    "        self.pred = Predictor(hidden_units=hidden_units,\n",
    "                              classes=self.num_emb)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        #encoding\n",
    "        out, state= self.enc(x,c)\n",
    "\n",
    "        #parametrization\n",
    "        z, mu, log_var = self.param(state[0])\n",
    "\n",
    "        #decoding\n",
    "        out = self.dec(z,c)\n",
    "\n",
    "        return self.pred(out).argmax(dim=-1), mu, log_var\n",
    "\n",
    "    def sample(self, z,c):\n",
    "        c = torch.nn.functional.interpolate(c.unsqueeze(1), size=(self.seq_len, self.cond_dim), mode='nearest').squeeze(1)\n",
    "        out = self.dec(z,c)\n",
    "        return self.pred(out).argmax(dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMVAE(nn.Module):\n",
    "    \"\"\"LSTM-based Variational Auto Encoder\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, seq_size, cond_size, hidden_size, latent_size, device=torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "    ):\n",
    "        \"\"\"\n",
    "        input_size: int, batch_size x sequence_length x input_dim\n",
    "        hidden_size: int, output size of LSTM AE\n",
    "        latent_size: int, latent z-layer size\n",
    "        num_lstm_layer: int, number of layers in LSTM\n",
    "        \"\"\"\n",
    "        super(LSTMVAE, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        # dimensions\n",
    "        self.input_size = seq_size + cond_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.num_layers = 1\n",
    "\n",
    "        #embeddings\n",
    "        self.emb = nn.Embedding(num_embeggings = self.vocab_size, embedding_dim=self.embed_size)\n",
    "        # lstm ae\n",
    "        self.lstm_enc = Encoder(\n",
    "            seq_size=seq_size, cond_size = cond_size, hidden_size=hidden_size, num_layers=self.num_layers\n",
    "        )\n",
    "        self.lstm_dec = Decoder(\n",
    "            input_size=latent_size + cond_size,\n",
    "            output_size=seq_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "\n",
    "        self.fc21 = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc22 = nn.Linear(self.hidden_size, self.latent_size)\n",
    "        self.fc3 = nn.Linear(self.latent_size, self.hidden_size)\n",
    "\n",
    "    def reparametize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        noise = torch.randn_like(std)\n",
    "\n",
    "        z = mu + noise * std\n",
    "        return z\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        batch_size, seq_len, feature_dim = x.shape\n",
    "\n",
    "        # encode input space to hidden space\n",
    "        enc_hidden = self.lstm_enc(x, c)\n",
    "        enc_h = enc_hidden[0].view(batch_size, self.hidden_size)\n",
    "\n",
    "        # extract latent variable z(hidden space to latent space)\n",
    "        mean = self.fc21(enc_h)\n",
    "        logvar = self.fc22(enc_h)\n",
    "        z = self.reparametize(mean, logvar)  # batch_size x latent_size\n",
    "\n",
    "        # initialize hidden state as inputs\n",
    "        h_ = self.fc3(z).unsqueeze(0)\n",
    "\n",
    "        # decode latent space to input space\n",
    "        z = z.repeat(1, seq_len, 1)\n",
    "        z = z.view(batch_size, seq_len, self.latent_size)\n",
    "\n",
    "        # initialize hidden state\n",
    "        hidden = (h_.contiguous(), h_.contiguous())\n",
    "        reconstruct_output, hidden = self.lstm_dec(z,c,hidden)\n",
    "\n",
    "        x_hat = reconstruct_output\n",
    "\n",
    "        # calculate vae loss\n",
    "        losses = self.loss_function(x_hat, x, mean, logvar)\n",
    "        m_loss, recon_loss, kld_loss = (\n",
    "            losses[\"loss\"],\n",
    "            losses[\"Reconstruction_Loss\"],\n",
    "            losses[\"KLD\"],\n",
    "        )\n",
    "\n",
    "        return x_hat, m_loss\n",
    "\n",
    "    def loss_function(self, *args, **kwargs) -> dict:\n",
    "        \"\"\"\n",
    "        Computes the VAE loss function.\n",
    "        KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "        :param args:\n",
    "        :param kwargs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "\n",
    "        kld_weight = 0.00025  # Account for the minibatch samples from the dataset\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "\n",
    "        kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + log_var - mu**2 - log_var.exp(), dim=1), dim=0\n",
    "        )\n",
    "\n",
    "        loss = recons_loss + kld_weight * kld_loss\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"Reconstruction_Loss\": recons_loss.detach(),\n",
    "            \"KLD\": -kld_loss.detach(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256]) torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "out = cvae(x,c,l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(lengths, maxlen, dtype=torch.int32):\n",
    "    if maxlen is None:\n",
    "        maxlen = lengths.max()\n",
    "    mask = ~(torch.ones((len(lengths), maxlen)).cumsum(dim=1).t() > lengths).t()\n",
    "    mask.type(dtype)\n",
    "    return mask\n",
    "\n",
    "def get_losses(y_hat, y, l, mu, logvar, kld_weight=0.0025):\n",
    "    #weight = sequence_mask(l,y.shape[1])\n",
    "    #weight = torch.randint(0,1,(120,4))\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    #print(y_hat.shape, torch.permute(y_hat,(0,2,1)).shape, y.shape, weight.shape)\n",
    "    recons_loss = loss(torch.permute(y_hat,(0,2,1)), y)\n",
    "    kld_loss = torch.mean(\n",
    "            -0.5 * torch.sum(1 + logvar - mu**2 - logvar.exp(), dim=1), dim=0\n",
    "        )\n",
    "    final_loss = recons_loss + kld_weight * kld_loss\n",
    "\n",
    "    return recons_loss, kld_loss, final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.5558, grad_fn=<NllLoss2DBackward0>),\n",
       " tensor(5.7151, grad_fn=<MeanBackward1>),\n",
       " tensor(3.5701, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_losses(out[0],y,l,out[2],out[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
